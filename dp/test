# -*- coding: utf-8 -*-
import scrapy
from scrapy.conf import settings
from dp.items import DpItem
from scrapy.selector import Selector
import os
os.environ["http_proxy"] = "http://qx11939:Ming1204@172.19.194.60:8080"

class DpshopSpider(scrapy.Spider):
    name = 'dpshop'
    allowed_domains = ['dianping.com']
    #start_urls = ['http://www.dianping.com/shanghai/ch10/g110p%s' % p for p in range(1, 3)]
    #start_urls =
    meta = {'dont_redirect': True,  # 禁止网页重定向
            'allow_redirects':False,
            'handle_httpstatus_list': [301, 302]}

    #cookie = settings['COOKIE']
    headers = {
    'Host': 'www.dianping.com',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate, sdch',
    'Connection': 'keep-alive',
}
    # foodtype = ['g1783','g110']
    # region = ['r3','r6']

    foodtype = ['g508']
    region = ['r6']
    # def start_requests(self): # 重构start_requests方法
    #     yield scrapy.Request(self.start_urls[0],callback=self.parse,cookies=self.cookie)

    def start_requests(self):
        # cookies = {'_lxsdk_cuid': '164b0658837c8-0ea560583c0c68-6b1b1279-1fa400-164b065883762', '_lxsdk': '164b0658837c8-0ea560583c0c68-6b1b1279-1fa400-164b065883762', '_hc.v': '7b174aa1-e83d-1c28-5eb3-d4a656dae863.1531967802', '__utma': '205923334.1273454000.1531968483.1531968483.1531968483.1', '__utmz': '205923334.1531968483.1.1.utmcsr', 'ctu': '383b4eeb970a809dd2f534730b186b40e6ab09579b4b72d0531260ca8b2ba56a', '_lx_utm': 'utm_source%3Ddp_pc_pet', 'cy': '1', 'cye': 'shanghai', 's_ViewType': '10', 'lgtoken': '04858c4ea-f696-43af-899a-fc820076aa0e', 'dper': 'a51fe1be04311badcb3fb5d4a6b08996593895a787c691eef3b0d9befa136d786a7f8cce06b979a95d481a63aaa1960b533ecd690e3505dc65111180a7d71e15bdfb7051958a21e90fe437e9f37c4d0fea0563503e977f7d7717f0077bc714e8', 'll': '7fd06e815b796be3df069dec7836c3df', 'ua': 'mInQ_532'}
        #url_basic = 'http://www.dianping.com/shanghai/ch10/'
        for food in self.foodtype:
            for qu in self.region:
                url = 'www.dianping.com/shanghai/ch10/%s%s' % (food,qu)
                #yield scrapy.Request(url,callback=self.parse_list,cookies = self.cookie,headers=self.headers,meta=self.meta)
                yield scrapy.Request(url, callback=self.parse_list,meta=self.meta)
            # yield scrapy.Request(url,callback=self.parse,cookies = self.cookie,headers=self.headers,meta=self.meta)
            # print(self.cookie)
    def parse_list(self,response):
        selector = Selector(response)

        pg = 0

        pages = selector.xpath('//div[@class="page"]/a/@data-ga-page').extract()

        #pg = int(str(pages))+1
        if len(pages) > 0:
            pg = pages[len(pages) - 2]

        pg = int(str(pg))+1

        url = str(response.url)

        for p in range(1,pg):
            ul = url+'p'+str(p)
            yield scrapy.Request(ul,callback=self.parse)


    def parse(self, response):
        # shop_page = '//*[@id="shop-all-list"]/ul/li/div[2]/div[1]/a[1]'
        # address_info = '//*[@id="shop-all-list"]/ul/li/div[2]/div[3]/span'
        for i in range(1,16):
            info = DpItem()
            shop_page_href = response.xpath('//*[@id="shop-all-list"]/ul/li'+'['+str(i)+']'+'/'+'div[2]/div[1]/a[1]/@href').extract()[0]
            info['url'] = response.xpath('//*[@id="shop-all-list"]/ul/li'+'['+str(i)+']'+'/'+'div[2]/div[1]/a[1]/@href').extract()[0]
            info['address'] = response.xpath('//*[@id="shop-all-list"]/ul/li'+'['+str(i)+']'+'/'+'div[2]/div[3]/span/text()').extract()[0]
            yield scrapy.Request(shop_page_href, callback=self.parse_next ,dont_filter=False,meta={'items':info})


    def parse_next(self, response):
        info = response.request.meta['items']
        try:
            info['city'] = response.xpath('//*[@id="logo-input"]/div/a[2]/span[2]/text()').extract()[0]
        except IndexError:
            info['city'] = ''
        else:
            info['city'] = response.xpath('//*[@id="logo-input"]/div/a[2]/span[2]/text()').extract()[0]
        info['name'] = response.xpath('//*[@id="body"]/div/div[1]/span/text()').extract()[0]
        info['district'] = response.xpath('//*[@id="body"]/div/div[1]/a[3]/text()').extract()[0]
        info['plate'] = response.xpath('//*[@id="body"]/div/div[1]/a[4]/text()').extract()[0]
        info['type'] = response.xpath('//*[@id="body"]/div/div[1]/a[2]/text()').extract()[0]
        info['comsume'] = response.xpath('//*[@id="avgPriceTitle"]/text()').extract()[0]
        info['taste'] = response.xpath('//*[@id="comment_score"]/span[1]/text()').extract()[0]
        info['envir'] = response.xpath('//*[@id="comment_score"]/span[2]/text()').extract()[0]
        info['service'] = response.xpath('//*[@id="comment_score"]/span[3]/text()').extract()[0]
        try:
            info['classify'] = response.xpath('//*[@id="cate-channel"]/div[1]/div/span/text()').extract()[0]
        except IndexError:
            info['classify'] = ''
        else:
            info['classify'] = response.xpath('//*[@id="cate-channel"]/div[1]/div/span/text()').extract()[0]

        try:
            info['company'] = response.xpath('//*[@class="safetyUl"]/li[2]/text()').extract()[0]
        except IndexError:
            info['company'] = ''
        else:
            info['company'] = response.xpath('//*[@class="safetyUl"]/li[2]/text()').extract()[0]

        try:
            info['company_address'] = response.xpath('//*[@class="safetyUl"]/li[4]/text()').extract()[0]
        except IndexError:
            info['company_address'] = ''
        else:
            info['company_address'] = response.xpath('//*[@class="safetyUl"]/li[4]/text()').extract()[0]

        try:
            info['company_type'] = response.xpath('//*[@class="safetyUl"]/li[5]/text()').extract()[0]
        except IndexError:
            info['company_type'] = ''
        else:
            info['company_type'] = response.xpath('//*[@class="safetyUl"]/li[5]/text()').extract()[0]

        try:
            info['company_style'] = response.xpath('//*[@class="safetyUl"]/li[6]/text()').extract()[0]
        except IndexError:
            info['company_style'] = ''
        else:
            info['company_style'] = response.xpath('//*[@class="safetyUl"]/li[6]/text()').extract()[0]

        yield info

